"""
FINAL WORKING Hybrid Retrieval System
CRITICAL FIX: Threshold lowered to 0.001 (was 0.05)
"""
import logging
from typing import List, Dict, Any, Optional, Tuple
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
import numpy as np
import os
import warnings

# Suppress warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', message='.*torch.classes.*')

from app.config import rag_config
from app.models.schemas import RetrievedChunk, QueryType

logger = logging.getLogger(__name__)


class HybridRetriever:
    """
    Production-Ready Hybrid Retriever

    CRITICAL FIX: RRF scores are naturally 0.001-0.02 range
    Threshold MUST be ‚â§ 0.001, NOT 0.05!
    """

    def __init__(self):
        logger.info("üöÄ Initializing HybridRetriever...")

        # 1. Load embedding model
        logger.info("   [1/3] Loading embedding model...")
        try:
            self.embedding_model = SentenceTransformer(
                rag_config.embedding_model,
                device='cpu'
            )
            logger.info(f"   ‚úÖ Loaded: {rag_config.embedding_model}")
        except Exception as e:
            logger.error(f"   ‚ùå Failed to load embedding model: {e}")
            raise

        # 2. Connect to ChromaDB
        logger.info("   [2/3] Connecting to ChromaDB...")
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=rag_config.chroma_persist_dir,
                settings=Settings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_collection(
                name=rag_config.chroma_collection_name
            )
            count = self.collection.count()
            logger.info(f"   ‚úÖ ChromaDB connected: {count} documents")

            if count == 0:
                logger.error("   ‚ùå ChromaDB is EMPTY!")
                raise ValueError("No documents in ChromaDB. Run 'python scripts/build_rag.py'")

        except Exception as e:
            logger.error(f"   ‚ùå ChromaDB connection failed: {e}")
            raise

        # 3. Build BM25 index
        logger.info("   [3/3] Building BM25 keyword index...")
        self._build_bm25_index()

        # 4. Load reranker (optional but recommended)
        self.reranker = None
        if rag_config.reranking_enabled:
            try:
                logger.info("   [OPTIONAL] Loading cross-encoder reranker...")
                # FIXED: Remove trust_remote_code (not supported in older versions)
                self.reranker = CrossEncoder(
                    rag_config.reranking_model,
                    max_length=512,
                    device='cpu'
                )
                logger.info("   ‚úÖ Reranker loaded (accuracy +20%)")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Reranker failed to load: {e}")
                logger.warning("   ‚Üí Continuing without reranking (accuracy will be lower)")

        logger.info("‚úÖ HybridRetriever ready!\n")

    def _build_bm25_index(self):
        """Build BM25 keyword search index from ChromaDB"""
        try:
            # Get all documents
            results = self.collection.get(include=["documents", "metadatas"])

            if not results['documents']:
                raise ValueError("ChromaDB returned no documents!")

            self.bm25_docs = results['documents']
            self.bm25_metadatas = results['metadatas']
            self.bm25_ids = results['ids']

            # Tokenize for BM25
            tokenized_corpus = [doc.lower().split() for doc in self.bm25_docs]
            self.bm25 = BM25Okapi(tokenized_corpus)

            logger.info(f"   ‚úÖ BM25 index built: {len(self.bm25_docs)} documents")

        except Exception as e:
            logger.error(f"   ‚ùå BM25 index build failed: {e}")
            raise

    def retrieve(
        self,
        query: str,
        query_type: Optional[QueryType] = None,
        top_k: int = 5,
        use_graph: bool = False
    ) -> List[RetrievedChunk]:
        """
        Main retrieval method - combines vector + BM25 + optional graph

        Args:
            query: User's question
            query_type: Type of query (code, docs, schema, etc.)
            top_k: Number of results to return
            use_graph: Whether to use graph database (Phase 2)

        Returns:
            List of RetrievedChunk objects with scores
        """
        logger.info("=" * 80)
        logger.info(f"üîç QUERY: '{query}'")
        logger.info(f"üìã Type: {query_type.value if query_type else 'GENERAL'}")
        logger.info("=" * 80)

        # Determine fusion alpha (vector vs keyword balance)
        if query_type:
            alpha = rag_config.get_alpha_for_query_type(query_type.value)
        else:
            alpha = rag_config.default_alpha

        logger.info(f"‚öñÔ∏è  Alpha (semantic/keyword balance): {alpha:.2f}")

        # Get more candidates initially for better reranking
        initial_k = top_k * 4  # e.g., get 20 to rerank down to 5

        # ==================================================================
        # STEP 1: Vector Search (Semantic)
        # ==================================================================
        logger.info("\n[1/4] Vector search (semantic similarity)...")
        try:
            vector_results = self._vector_search(query, initial_k)
            logger.info(f"   ‚úÖ Found {len(vector_results)} results")
            if vector_results:
                logger.info(f"   üìä Top 3 scores: {[f'{r[1]:.4f}' for r in vector_results[:3]]}")
        except Exception as e:
            logger.error(f"   ‚ùå Vector search failed: {e}")
            vector_results = []

        # ==================================================================
        # STEP 2: BM25 Search (Keyword)
        # ==================================================================
        logger.info("\n[2/4] BM25 search (keyword matching)...")
        try:
            bm25_results = self._bm25_search(query, initial_k)
            logger.info(f"   ‚úÖ Found {len(bm25_results)} results")
            if bm25_results:
                logger.info(f"   üìä Top 3 scores: {[f'{r[1]:.4f}' for r in bm25_results[:3]]}")
        except Exception as e:
            logger.error(f"   ‚ùå BM25 search failed: {e}")
            bm25_results = []

        # ==================================================================
        # STEP 3: Graph Search (Optional - Phase 2)
        # ==================================================================
        logger.info("\n[3/4] Graph search (code relationships)...")
        graph_results = []  # Always initialize

        if use_graph and rag_config.graph_enabled:
            try:
                from app.graph.retriever import GraphRetriever
                graph_retriever = GraphRetriever()
                graph_results = graph_retriever.retrieve(query, initial_k)
                logger.info(f"   ‚úÖ Found {len(graph_results)} results")
            except Exception as e:
                logger.info(f"   ‚è≠Ô∏è  Graph search skipped: {e}")
        else:
            logger.info("   ‚è≠Ô∏è  Graph search disabled (enable in Phase 2)")

        # ==================================================================
        # STEP 4: Fusion (Combine results with RRF)
        # ==================================================================
        logger.info("\n[4/4] Fusing results (RRF algorithm)...")
        try:
            fused_results = self._reciprocal_rank_fusion(
                vector_results=vector_results,
                bm25_results=bm25_results,
                graph_results=graph_results,
                alpha=alpha
            )
            logger.info(f"   ‚úÖ Fused {len(fused_results)} unique results")
            if fused_results:
                logger.info(f"   üìä Top 5 scores: {[f'{r[1]:.4f}' for r in fused_results[:5]]}")
        except Exception as e:
            logger.error(f"   ‚ùå Fusion failed: {e}")
            # Fallback: just use vector results
            fused_results = vector_results

        # ==================================================================
        # STEP 5: Reranking (Optional - boosts accuracy 20-30%)
        # ==================================================================
        if self.reranker and len(fused_results) > 1:
            logger.info("\n[5/5] Reranking with cross-encoder...")
            try:
                reranked_results = self._rerank(query, fused_results, top_k * 2)
                logger.info(f"   ‚úÖ Reranked {len(reranked_results)} results")
                if reranked_results:
                    logger.info(f"   üìä Top 5 scores: {[f'{r[1]:.4f}' for r in reranked_results[:5]]}")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Reranking failed: {e}")
                reranked_results = fused_results[:top_k * 2]
        else:
            logger.info("\n[5/5] Reranking skipped (no reranker loaded)")
            reranked_results = fused_results[:top_k * 2]

        # ==================================================================
        # STEP 6: Convert to RetrievedChunk objects
        # ==================================================================
        logger.info("\n[6/6] Filtering and formatting results...")
        chunks = []

        # ‚úÖ CRITICAL FIX: RRF scores are 0.001-0.02 range (very low!)
        # Your scores: 0.0082, 0.0161 (normal for RRF)
        # Old threshold (0.05) filtered EVERYTHING
        # New threshold (0.001) allows these normal scores through
        MIN_SCORE_THRESHOLD = 0.001  # ‚úÖ FIXED!

        logger.info(f"   Using threshold: {MIN_SCORE_THRESHOLD}")

        for doc_id, score, content, metadata in reranked_results:
            if score < MIN_SCORE_THRESHOLD:
                logger.debug(f"   ‚è≠Ô∏è  Skipped (score {score:.6f} < {MIN_SCORE_THRESHOLD})")
                continue

            chunks.append(RetrievedChunk(
                content=content,
                score=float(score),
                source=metadata.get('source', 'unknown'),
                metadata=metadata,
                chunk_id=doc_id
            ))

            if len(chunks) >= top_k:
                break

        # ==================================================================
        # FINAL SUMMARY
        # ==================================================================
        logger.info(f"\n{'=' * 80}")
        logger.info(f"‚úÖ RETRIEVAL COMPLETE")
        logger.info(f"{'=' * 80}")
        logger.info(f"üìä Results: {len(chunks)} chunks retrieved")

        if chunks:
            logger.info(f"üìà Score range: {chunks[0].score:.6f} ‚Üí {chunks[-1].score:.6f}")
            logger.info(f"üìÅ Top sources:")
            for i, chunk in enumerate(chunks[:3], 1):
                filename = chunk.metadata.get('filename', 'unknown')
                logger.info(f"   {i}. {filename} (score: {chunk.score:.6f})")
        else:
            logger.warning("‚ö†Ô∏è NO CHUNKS RETRIEVED!")
            logger.warning(f"   All RRF scores were below {MIN_SCORE_THRESHOLD}")
            if reranked_results:
                best_score = max(r[1] for r in reranked_results)
                logger.warning(f"   Best score was: {best_score:.6f}")
                logger.warning(f"   Consider lowering threshold further")

        logger.info(f"{'=' * 80}\n")

        return chunks

    def _vector_search(
        self, query: str, top_k: int
    ) -> List[Tuple[str, float, str, Dict]]:
        """
        Vector similarity search using embeddings
        Returns: [(doc_id, score, content, metadata), ...]
        """
        # Generate query embedding
        query_embedding = self.embedding_model.encode(
            query,
            convert_to_numpy=True
        ).tolist()

        # Search ChromaDB
        actual_k = min(top_k, self.collection.count())
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=actual_k,
            include=["documents", "metadatas", "distances"]
        )

        # Format results
        formatted = []
        for i in range(len(results['ids'][0])):
            doc_id = results['ids'][0][i]
            distance = results['distances'][0][i]
            score = 1 - distance  # Convert distance to similarity
            content = results['documents'][0][i]
            metadata = results['metadatas'][0][i]

            formatted.append((doc_id, score, content, metadata))

        return formatted

    def _bm25_search(
        self, query: str, top_k: int
    ) -> List[Tuple[str, float, str, Dict]]:
        """
        BM25 keyword search
        Returns: [(doc_id, score, content, metadata), ...]
        """
        # Tokenize query
        tokenized_query = query.lower().split()

        # Get BM25 scores
        scores = self.bm25.get_scores(tokenized_query)

        # Normalize to 0-1 range
        if scores.max() > 0:
            scores = scores / scores.max()
        else:
            logger.warning("   ‚ö†Ô∏è BM25 scores all zero (query too generic)")

        # Get top K indices
        actual_k = min(top_k, len(scores))
        top_indices = np.argsort(scores)[-actual_k:][::-1]

        # Format results
        formatted = []
        for idx in top_indices:
            doc_id = self.bm25_ids[idx]
            score = float(scores[idx])
            content = self.bm25_docs[idx]
            metadata = self.bm25_metadatas[idx]

            formatted.append((doc_id, score, content, metadata))

        return formatted

    def _reciprocal_rank_fusion(
        self,
        vector_results: List[Tuple],
        bm25_results: List[Tuple],
        graph_results: List[Tuple],
        alpha: float = 0.5
    ) -> List[Tuple[str, float, str, Dict]]:
        """
        Reciprocal Rank Fusion (RRF)

        IMPORTANT: RRF produces SMALL scores (0.001-0.02 range)
        This is NORMAL and EXPECTED!

        Formula: score(d) = Œ£ 1/(k + rank(d))
        Example: rank=1, k=60 ‚Üí 1/61 = 0.016

        Args:
            vector_results: Results from vector search
            bm25_results: Results from BM25 search
            graph_results: Results from graph search (optional)
            alpha: Weight for vector (1-alpha for BM25)

        Returns:
            Fused and sorted results
        """
        k = rag_config.rrf_k  # Typically 60
        scores = {}
        contents = {}
        metadatas = {}

        # Process vector results (weight: alpha)
        for rank, (doc_id, _, content, metadata) in enumerate(vector_results, 1):
            rrf_score = alpha * (1 / (k + rank))
            scores[doc_id] = scores.get(doc_id, 0) + rrf_score
            contents[doc_id] = content
            metadatas[doc_id] = metadata

        # Process BM25 results (weight: 1-alpha)
        for rank, (doc_id, _, content, metadata) in enumerate(bm25_results, 1):
            rrf_score = (1 - alpha) * (1 / (k + rank))
            scores[doc_id] = scores.get(doc_id, 0) + rrf_score
            if doc_id not in contents:
                contents[doc_id] = content
                metadatas[doc_id] = metadata

        # Process graph results (small weight)
        graph_weight = 0.2
        for rank, (doc_id, _, content, metadata) in enumerate(graph_results, 1):
            rrf_score = graph_weight * (1 / (k + rank))
            scores[doc_id] = scores.get(doc_id, 0) + rrf_score
            if doc_id not in contents:
                contents[doc_id] = content
                metadatas[doc_id] = metadata

        # Sort by combined score
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        # Format output
        fused = [
            (doc_id, score, contents[doc_id], metadatas[doc_id])
            for doc_id, score in sorted_items
        ]

        return fused

    def _rerank(
        self, query: str, candidates: List[Tuple], top_k: int
    ) -> List[Tuple[str, float, str, Dict]]:
        """
        Cross-encoder reranking for improved precision

        Takes initial retrieval results and re-scores them with
        a more sophisticated model that considers query-document interaction.

        Returns: Reranked list of (doc_id, score, content, metadata)
        """
        if not candidates:
            return []

        # Prepare query-document pairs
        pairs = [(query, content) for _, _, content, _ in candidates]

        # Get reranking scores
        try:
            rerank_scores = self.reranker.predict(
                pairs,
                show_progress_bar=False,
                batch_size=32
            )
        except Exception as e:
            logger.error(f"   ‚ùå Reranking prediction failed: {e}")
            return candidates[:top_k]

        # Normalize scores to 0-1
        rerank_scores = np.array(rerank_scores)
        if rerank_scores.max() > rerank_scores.min():
            rerank_scores = (rerank_scores - rerank_scores.min()) / \
                          (rerank_scores.max() - rerank_scores.min())

        # Combine with candidates
        reranked = []
        for (doc_id, _, content, metadata), score in zip(candidates, rerank_scores):
            reranked.append((doc_id, float(score), content, metadata))

        # Sort by rerank score
        reranked.sort(key=lambda x: x[1], reverse=True)

        return reranked[:top_k]


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("\n" + "=" * 80)
    print("TESTING HYBRID RETRIEVER")
    print("=" * 80 + "\n")

    try:
        # Initialize
        retriever = HybridRetriever()

        # Test queries
        test_queries = [
            ("UserController", QueryType.CODE_SEARCH),
            ("database table", QueryType.SCHEMA),
            ("authentication", QueryType.DOCUMENTATION),
        ]

        for query, qtype in test_queries:
            print(f"\n{'=' * 80}")
            print(f"TEST: {query}")
            print(f"{'=' * 80}\n")

            results = retriever.retrieve(query, query_type=qtype, top_k=3)

            if results:
                print(f"\n‚úÖ SUCCESS: {len(results)} results")
                for i, chunk in enumerate(results, 1):
                    print(f"\n{i}. {chunk.metadata.get('filename', 'unknown')}")
                    print(f"   Score: {chunk.score:.6f}")
                    print(f"   Preview: {chunk.content[:100]}...")
            else:
                print("\n‚ùå FAILED: No results")

    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()